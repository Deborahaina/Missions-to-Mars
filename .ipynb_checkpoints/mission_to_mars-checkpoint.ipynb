{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "from splinter import Browser\n",
    "from splinter.exceptions import ElementDoesNotExist\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = {'executable_path': 'C:\\\\Users\\\\debor\\\\GWARL201811DATA3\\\\01-Class-Activities\\\\12-Web-Scraping-and-Document-Databases\\\\chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.visit('https://mars.nasa.gov/news/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = browser.html\n",
    "soup = BeautifulSoup(html,\"html.parser\")\n",
    "missions = soup.find_all(\"div\", class_=\"content_title\")\n",
    "missions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for mission in missions:\n",
    "    try:\n",
    "    \n",
    "        news_title= mission.a.text\n",
    "        print(news_title)\n",
    "        \n",
    "    except ElementDoesNotExist as exception:\n",
    "        print(\"Element does not Exist\")\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape each news article's paragraph\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html,\"html.parser\")\n",
    "news_p= soup.find_all(\"div\", class_=\"article_teaser_body\")\n",
    "news_p[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for p in news_p:\n",
    "    try: \n",
    "        news_p=p.text\n",
    "        print(news_p)\n",
    "        #news_p_list.append(paragraph)\n",
    "        \n",
    "    except ElementDoesNotExist as exception:\n",
    "        print(\"Element Does not Exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JPL Mars Space Images - Featured Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit Mars Space Images through selenium module\n",
    "url= 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse through the url and split it to dereive the base url\n",
    "from urllib.parse import urlsplit\n",
    "base_url = \"{0.scheme}://{0.netloc}/\".format(urlsplit(url))\n",
    "print(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpath = \"//*[@id=\\\"page\\\"]/section[3]/div/ul/li[1]/a/div/div[2]/img\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = browser.find_by_xpath(xpath)\n",
    "img = results[0]\n",
    "img.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_image = browser.html\n",
    "\n",
    "soup = BeautifulSoup(html_image, \"html.parser\")\n",
    "\n",
    "img_url = soup.find(\"img\", class_=\"fancybox-image\")[\"src\"]\n",
    "\n",
    "full_img_url = base_url + img_url\n",
    "\n",
    "print(full_img_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping Mars Weather\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://twitter.com/marswxreport?lang=en'\n",
    "browser.visit(url)\n",
    "# relative path //p[contains(text(),'InSight sol 86 (2019-02-22) low -95.3ºC (-139.5ºF)')]\n",
    "#abosulte path //*[@id=\"stream-item-tweet-1100190498889912321\"]/div[1]/div[2]/div[2]/p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_weather = browser.html\n",
    "soup = BeautifulSoup(html_weather,\"html.parser\")\n",
    "\n",
    "mars_weather= soup.find_all(\"p\", class_=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in mars_weather:\n",
    "    try: \n",
    "        mars_weather=m.text\n",
    "        \n",
    "        print(mars_weather)\n",
    "        \n",
    "    except ElementDoesNotExist as exception:\n",
    "        print(\"Element Does not Exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mars Fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_facts='https://space-facts.com/mars/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.read_html(url_facts)\n",
    "table[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename Columns and set index\n",
    "df_mars_facts = table[0]\n",
    "df_mars_facts.columns = [\"Parameter\", \"Values\"]\n",
    "df_mars_facts.set_index([\"Parameter\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to html table \n",
    "\n",
    "mars_html_table = df_mars_facts.to_html()\n",
    "mars_html_table = mars_html_table.replace(\"\\n\", \"\")\n",
    "mars_html_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mars Hemisphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_hemisphere= 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "browser.visit(url_hemisphere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hemisphere_base_url = \"{0.scheme}://{0.netloc}/\".format(urlsplit(url_hemisphere))\n",
    "print(hemisphere_base_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cerberus-Hemisphere-image-url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cerberus_image = browser.html\n",
    "#xpath=//img[@class='wide-image']\n",
    "\n",
    "html_hemispheres = browser.html\n",
    "\n",
    "#xpath for everything mars hemisphere= //section[@class='block']//div[1]//div[1]//div[1]\n",
    "\n",
    "\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup = BeautifulSoup(html_hemispheres, 'html.parser')\n",
    "\n",
    "# Retreive all items that contain mars hemispheres information\n",
    "items = soup.find_all('div', class_='item')\n",
    "\n",
    "# Create empty list for hemisphere urls \n",
    "hemisphere_image_urls = []\n",
    "\n",
    "# Store the main_ul \n",
    "hemispheres_main_url = 'https://astrogeology.usgs.gov'\n",
    "\n",
    "# Loop through the items previously stored\n",
    "for i in items: \n",
    "    # Store title\n",
    "    title = i.find('h3').text\n",
    "    \n",
    "    # Store link that leads to full image website\n",
    "    partial_img_url = i.find('a', class_='itemLink product-item')['href']\n",
    "    \n",
    "    # Visit the link that contains the full image website \n",
    "    browser.visit(hemispheres_main_url + partial_img_url)\n",
    "    \n",
    "    # HTML Object of individual hemisphere information website \n",
    "    partial_img_html = browser.html\n",
    "    \n",
    "    # Parse HTML with Beautiful Soup for every individual hemisphere information website \n",
    "    soup = BeautifulSoup( partial_img_html, 'html.parser')\n",
    "    \n",
    "    # Retrieve full image source \n",
    "    img_url = hemispheres_main_url + soup.find('img', class_='wide-image')['src']\n",
    "    \n",
    "    # Append the retreived information into a list of dictionaries \n",
    "    hemisphere_image_urls.append({\"title\" : title, \"img_url\" : img_url})\n",
    "    \n",
    "\n",
    "# Display hemisphere_image_urls\n",
    "hemisphere_image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
